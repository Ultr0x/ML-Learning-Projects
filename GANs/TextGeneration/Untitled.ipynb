{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ca5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09eceecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function reweight_distribution at 0x111a62050>\n"
     ]
    }
   ],
   "source": [
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature \n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)\n",
    "print(reweight_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18bed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 18:23:38.184295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 18:24:06.103437: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828fbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3437c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "vocab_size = 15000\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72844ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_dataset(text_batch):\n",
    "    vectorized_sequences = text_vectorization(text_batch)\n",
    "    x = vectorized_sequences[:, :-1] \n",
    "    y = vectorized_sequences[:, 1:] \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "532340e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91367e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers \n",
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7538754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1) \n",
    "        embedded_tokens = self.token_embeddings(inputs) \n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "    def compute_mask(self, inputs, mask=None): \n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "    def get_config(self):\n",
    "        config = super().get_config() \n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim, \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fabb016",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformerDecoder\u001b[39;00m(layers\u001b[38;5;241m.\u001b[39mLayer):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, dense_dim, num_heads, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn [29], line 15\u001b[0m, in \u001b[0;36mTransformerDecoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_2 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mMultiHeadAttention(\n\u001b[1;32m     10\u001b[0m             num_heads\u001b[38;5;241m=\u001b[39mnum_heads, key_dim\u001b[38;5;241m=\u001b[39membed_dim)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_proj \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     12\u001b[0m             [layers\u001b[38;5;241m.\u001b[39mDense(dense_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     13\u001b[0m      layers\u001b[38;5;241m.\u001b[39mDense(embed_dim),]\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mlayernorm_1 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mLayerNormalization()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_2 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mLayerNormalization()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_3 \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mLayerNormalization()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.dense_dim = dense_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.attention_1 = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=embed_dim)\n",
    "            self.attention_2 = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=embed_dim)\n",
    "            self.dense_proj = keras.Sequential(\n",
    "                [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "         layers.Dense(embed_dim),]\n",
    "    )\n",
    "    self.layernorm_1 = layers.LayerNormalization()\n",
    "    self.layernorm_2 = layers.LayerNormalization()\n",
    "    self.layernorm_3 = layers.LayerNormalization()\n",
    "    self.supports_masking = True\n",
    "    def get_config(self):\n",
    "        config = super().get_config() \n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb41330f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerDecoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n\u001b[0;32m----> 3\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerDecoder\u001b[49m(embed_dim, latent_dim, num_heads)(x, x)\n\u001b[1;32m      4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(vocab_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerDecoder' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs) \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
